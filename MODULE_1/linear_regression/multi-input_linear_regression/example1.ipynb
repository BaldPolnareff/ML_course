{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi input linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a single input to predict a single output, we can use multiple inputs to predict a single output. This is called multi input linear regression. \n",
    "\n",
    "The house prices (our targets $y$) can be predicted not only by a single feature (the size of the house) but also by the number of bedrooms, floors, its age, etc. \n",
    "\n",
    "In this context the bias $b$ can be thought of as a baseline price for a house with no assumptions about its size, number of bedrooms, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a random example: \n",
    "\n",
    "$$ f_{wb}(\\vec{x}) = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + b $$\n",
    "\n",
    "where $\\vec{x} = [x_1, x_2, x_3, x_4]$ is a vector of features and $w_1, w_2, w_3, w_4$ are the weights for each feature. \n",
    "\n",
    "For instance: \n",
    "\n",
    "$$ f_{wb}(\\vec{x}) = 0.1 x_1 + 4 x_2 + 10 x_3 + (-2) x_4 + 80 $$\n",
    "\n",
    "where $x_1$ is the size of the house, $x_2$ is the number of bedrooms, $x_3$ is the number of floors, $x_4$ is the age of the house and $80$ is the baseline price for a house with no assumptions about its size, number of bedrooms, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That simply translates to the idea that the first feature (the size of the house), has a weight of 0.1 in determining the price of the house, the second feature (the number of bedrooms) has a weight of 4, etc, and the baseline price is $80000 \\ USD$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generalize for $n$ features:\n",
    "\n",
    "$$ f_{wb}(\\vec{x}) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b $$\n",
    "\n",
    "where $\\vec{x} = [x_1, x_2, ..., x_n]$ is a vector of features and $w_1, w_2, ..., w_n$ are the weights for each feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a more compact notation:\n",
    "\n",
    "$$ f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b $$\n",
    "\n",
    "where $\\vec{w} = [w_1, w_2, ..., w_n]$ is a vector of weights and $\\vec{x} = [x_1, x_2, ..., x_n]$ is a vector of features.\n",
    "\n",
    "Remember that a scalar product of two vectors outputs a scalar, so the output of the function is a scalar, which is in line with what we want to achieve: a single output, signifying the predicted price of the house $\\hat{y}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this notation is very clear and concise, in terms of specifying the types of inputs and outputs of the function as either vectors or scalars. \n",
    "\n",
    "We might simplify this notation in the future, if the context makes clear what's a vector and what's a scalar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent \n",
    "\n",
    "We can use gradient descent to find the optimal weights and bias for our multi input linear regression model. \n",
    "\n",
    "We can use the same cost function as before, but we need to update the partial derivatives to account for the fact that we have multiple inputs, by introducing the vector notation. \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i} J(\\vec{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} 2(\\vec{w} \\cdot \\vec{x}^{(i)} + b - y^{(i)}) x_i^{(i)} $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b} J(\\vec{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} 2(\\vec{w} \\cdot \\vec{x}^{(i)} + b - y^{(i)}) $$\n",
    "\n",
    "where $\\vec{w} = [w_1, w_2, ..., w_n]$ is a vector of weights and $\\vec{x} = [x_1, x_2, ..., x_n]$ is a vector of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using python notation, we can write the partial derivatives as follows, taking into account that these are assignments, not equality statements:  \n",
    "\n",
    "$$ w = w - \\alpha \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n} \\left(f_{w, b}(x^{(i)}) - y^{(i)}\\right) x^{(i)} $$\n",
    "\n",
    "$$ b = b - \\alpha \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n} \\left(f_{w, b}(x^{(i)}) - y^{(i)}\\right) $$\n",
    "\n",
    "where $\\alpha$ is the learning rate, to be chosen with care, as too small a value will result in a slow convergence, while too large a value will result in a divergence, also known as overshooting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is correct if and only if it takes into account a simultaneous update of all the weights and the bias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the update of the weights and the bias should be done in a single step, not in separate steps, for all elements of the vector $\\vec{w}$ and the scalar $b$, as follows: \n",
    "\n",
    "$$ w_1 = w_1 - \\alpha \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n} \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right) x_1^{(i)} $$\n",
    "$$ \\displaystyle\\vdots $$\n",
    "$$ w_n = w_n - \\alpha \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n} \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right) x_n^{(i)} $$\n",
    "\n",
    "$$ b = b - \\alpha \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n} \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative to gradient descent: the normal equation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm only works for linear regression, and it solves for $w$ and $b$ without the need to iterate.\n",
    "\n",
    "This method is also very slow for large numbers of features (> 10000)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It generally is implemented in machine learning libraries for linear regression, but can't be used for other models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appropriate features scaling\n",
    "\n",
    "When using gradient descent, it is important to scale the features appropriately, so that they are all in the same range. \n",
    "\n",
    "This can achieved by observing that different features can have vastly different ranges, and the weights can be sized accordingly, to compensate for this.\n",
    "\n",
    "A better approach is to scale the features so that they are all normalized to the same range, for instance between 0 and 1, or between -1 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of the learning rate $\\alpha$ \n",
    "\n",
    "The learning rate $\\alpha$ is a hyperparameter that needs to be chosen with care. \n",
    "\n",
    "If $\\alpha$ is too small, the algorithm will take a long time to converge, and if $\\alpha$ is too large, the algorithm will diverge, also known as overshooting. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve diagnostics\n",
    "\n",
    "The learning curve is a plot of the cost function as a function of the number of iterations of gradient descent. \n",
    "\n",
    "It is a useful tool to diagnose the performance of the algorithm, and to determine if the algorithm is converging, diverging or if it is stuck in a local minimum. \n",
    "\n",
    "Remember that the objective of the algorithm is to minimize the cost function, so the cost function should be decreasing as the number of iterations increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic convergence test \n",
    "\n",
    "Let $\\epsilon$ be a small positive number, and let $J(\\vec{w}, b)$ be the cost function. \n",
    "\n",
    "If the cost function $J(\\vec{w}, b)$ decreases by less than $\\epsilon$ between two consecutive iterations, then the algorithm has converged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c9d14e2283efc71dd845cb84a05c6dc0b4f1a825c12450a6974e74c8800d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
