{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go \n",
    "import seaborn as sns \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model a linear regression model to predict the price of a house based on its size, based on the following data:\n",
    "\n",
    "|`x`: **Size** [1000 sqft] |`y`: **Price** [$1000]  |\n",
    "|--------------------------|------------------------|\n",
    "|1.0                       |300.0                   |\n",
    "|2.0                       |500.0                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor([1.0, 2.0])\n",
    "y_train = torch.tensor([300.0, 500.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_wb(x, w, b): \n",
    "    return w * x + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 200\n",
    "b = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = f_wb(x_train, w, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the cost function\n",
    "\n",
    "In this situation, the cost function is the **mean squared error** (MSE) between the predicted and the actual values. The MSE is a suitable cost function for linear regression because it penalizes large errors. The MSE is defined as:\n",
    "\n",
    "$$ J(w, b) = \\displaystyle\\frac{1}{2m} \\sum_{i=0}^{m-1} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $m$ is the number of training examples, $y_i$ is the actual value of the $i$-th training example, and $\\hat{y}_i$ is the predicted value of the $i$-th training example, given by the linear function: \n",
    "\n",
    "$$ f_{w, b}(x_i) = w x_i + b $$\n",
    "\n",
    "where $w$ is the weight and $b$ is the bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([300., 500.]), tensor([300., 500.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactively plot the linear regression model and the cost function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot with two subfigures. The first subfigure will show the linear regression model and the training data. The second subfigure will show the cost function as a function of the weight and bias. The cost function will be computed for a grid of values of the weight and bias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a value of $b = 100$ provided an optimal solution from the previous exercise, we will fix it and only vary the weight $w$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a fixed bias and a range of values for the weights, as well as compute the linear regression and the cost function for each value of the weight, which we will connect to the slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(fun, w, b, x, y):\n",
    "    m = x.shape[0]\n",
    "    y_hat = fun(x, w, b)\n",
    "    cost = 1/(2*m) * torch.sum((y_hat - y)**2)\n",
    "    return cost\n",
    "\n",
    "def interact_plot_funcs(x_train, y_train):\n",
    "    X = torch.tensor(x_train)\n",
    "    Y = torch.tensor(y_train)\n",
    "\n",
    "    def plot_funcs(X, Y, w):\n",
    "        f_wb = lambda x, w, b: w*x + b\n",
    "        y_hat = f_wb(X, w, 100)\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "        axs[0].plot(X.numpy(), Y.numpy(), 'o')\n",
    "        axs[0].plot(X.numpy(), y_hat.numpy(), '-')\n",
    "        axs[0].set_title('Linear Regression')\n",
    "        axs[0].set_xlabel('Size [1000 ft²]')\n",
    "        axs[0].set_ylabel('Prices [1000 USD]')\n",
    "        axs[0].legend(['data points', 'linear regression'])\n",
    "\n",
    "        J_vals = torch.tensor([MSE(f_wb, w, 100, X, Y) for w in w_vals])\n",
    "        axs[1].plot(w_vals.numpy(), J_vals.numpy(), '-')\n",
    "        axs[1].scatter(w, MSE(f_wb, w, 100, X, Y), c='r', marker='o')\n",
    "        axs[1].set_title('Cost Function')\n",
    "        axs[1].set_xlabel('w')\n",
    "        axs[1].set_ylabel('J(w)')\n",
    "        axs[1].legend(['cost', 'current cost'])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    w_vals = torch.linspace(0, 400, 100)\n",
    "    interact = ipywidgets.interact(plot_funcs, X=ipywidgets.fixed(X), Y=ipywidgets.fixed(Y), w=(0, 400, 0.1))\n",
    "    return interact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_483183/2135573596.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(x_train)\n",
      "/tmp/ipykernel_483183/2135573596.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(y_train)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58830209435149a5b9746a7a3290b5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=200.0, description='w', max=400.0), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact_plot_funcs(x_train, y_train)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c9d14e2283efc71dd845cb84a05c6dc0b4f1a825c12450a6974e74c8800d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
