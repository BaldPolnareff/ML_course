{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression \n",
    "\n",
    "Similarly to linear regression, logistic regression can also be regularized. The regularization term is added to the cost function and has the same interpretation as before: it prevents overfitting the training data. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refresh our memory about the cost function of logistic regression: \n",
    "\n",
    "$$ \\displaystyle J(\\vec{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) \\right) + (1 - y^{(i)}) \\log \\left( 1 - f_{\\vec{w}, b}(\\vec{x}^{(i)}) \\right) \\right] $$\n",
    "\n",
    "where $f_{\\vec{w}, b}(\\vec{x}) = \\displaystyle \\frac{1}{1 + e^{-z}} $ is the sigmoid function, and $z$ is a function (linear or polynomial) of the input features $\\vec{x}$ and the weights $\\vec{w}$ and the bias $b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize it to take advantage of regularization:\n",
    "\n",
    "$$ \\displaystyle J(\\vec{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) \\right) + (1 - y^{(i)}) \\log \\left( 1 - f_{\\vec{w}, b}(\\vec{x}^{(i)}) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for linear regression, we can use gradient descent to find the optimal values of $\\vec{w}$ and $b$. \n",
    "\n",
    "The partial derivatives of the cost function with respect to $\\vec{w}$ and $b$ are: \n",
    "\n",
    "$$ \\displaystyle \\frac{\\partial J}{\\partial \\vec{w}} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\vec{x}^{(i)} + \\frac{\\lambda}{m} \\vec{w} $$ \n",
    "\n",
    "$$ \\displaystyle \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, **gradient descent** is implemented in code by simultaneously updating the values of $\\vec{w}$ and $b$, as follows: \n",
    "\n",
    "$$ \\vec{w} = \\vec{w} - \\alpha \\frac{\\partial J}{\\partial \\vec{w}} = \\vec{w} - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\vec{x}^{(i)} + \\frac{\\lambda}{m} \\vec{w} \\right) $$ \n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial J}{\\partial b} = b - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\right) $$\n",
    "\n",
    "where $\\alpha$ is the learning rate. \n",
    "\n",
    "Bear in mind that these are variable assignments, not equations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an auxiliary sigmoid function \n",
    "\n",
    "def sigmoid(fun, X, w, b):\n",
    "    z = fun(X, w, b)\n",
    "    return 1/(1 + np.e**(-1 * z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the updated cost function\n",
    "\n",
    "def J_logistic_reg(X, y, w, b, fun, lambda_=1): \n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      fun (function)  : function to be passed to a sigmoid function to perform (g(f()))\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape \n",
    "    cost = 0. \n",
    "    loss = 0. \n",
    "    reg_cost = 0. \n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = fun\n",
    "        y_hat_i = sigmoid(z_i, X[i], w, b)\n",
    "        loss += -y[i] * np.log(y_hat_i) - (1 - y[i]) * np.log(1 - y_hat_i)\n",
    "    loss /= m \n",
    "\n",
    "    for j in range(n): \n",
    "        reg_cost += (w[j]**2)\n",
    "    reg_cost *= (lambda_ / (2*m))\n",
    "\n",
    "    cost = loss + reg_cost\n",
    "    return cost  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this with some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(X, w, b): \n",
    "    return np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6850849138741673\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = J_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, fun=linear_reg, lambda_=lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also implement the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_, type_, fun_=linear_reg):\n",
    "  \"\"\"\n",
    "   Computes the gradient for linear or logistic regression  \n",
    "   Args:\n",
    "     X (ndarray (m,n)  : Data, m examples with n features\n",
    "     y (ndarray (m,))  : target values\n",
    "     w (ndarray (n,))  : model parameters  \n",
    "     b (scalar)        : model parameter\n",
    "     lambda_ (scalar)  : Controls amount of regularization\n",
    "     type_   (string)  : Either 'linreg' or 'logreg' \n",
    "     fun_    (function): function to be passed to a sigmoid function to perform (g(f()))\n",
    "   Returns\n",
    "     dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "     dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "   \"\"\" \n",
    "\n",
    "  m, n = X.shape\n",
    "  dj_dw = np.zeros((n, ))\n",
    "  dj_db = 0.0\n",
    "\n",
    "  if type_ not in ['logreg', 'linreg']:\n",
    "   raise ValueError(f'Incorrect value for \"type_\": {type_}. Please pick either \"linreg\" or \"logreg\". ')\n",
    "\n",
    "\n",
    "  def y_hatfun(X, w, b, TYPE):\n",
    "   if TYPE == 'linreg':\n",
    "      return linear_reg(X, w, b)\n",
    "   elif TYPE == 'logreg':\n",
    "      return sigmoid(fun_, X, w, b)    \n",
    "   \n",
    "  for i in range(m):\n",
    "     y_hat_i = y_hatfun(X[i], w, b, TYPE=type_)\n",
    "     err_i = y_hat_i - y[i]\n",
    "     for j in range(n):\n",
    "        dj_dw[j] += err_i * X[i, j]\n",
    "     dj_db += err_i\n",
    "  dj_dw /= m \n",
    "  dj_db /= m  \n",
    "  for j in range(n): \n",
    "     dj_dw[j] += (lambda_/m) * w[j]  \n",
    "  \n",
    "  return dj_db, dj_dw "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this with some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851497]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient(X_tmp, y_tmp, w_tmp, b_tmp, lambda_=lambda_tmp, type_='logreg')\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
