{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Regression \n",
    "\n",
    "The goal of regularized linear regression is to regulate the weights of the linear regression model to prevent overfitting. \n",
    "\n",
    "Overfitting is a problem that occurs when a model is too complex for the available data. In this case, the model will fit the training data very well, but will not generalize well to new data. \n",
    "\n",
    "A general rule of thumb is that the more data you have, the less regularization you need, but this is not always the case. You might have very little data, but a lot of features, not all of which are useful. This is a typical use case for regularization, as you can influence the model to focus on the most useful features. \n",
    "\n",
    "Another rule of thumb is that in the presence of a way too complex function, reducing all the weights provides a smoother fit, i.e. with a lower variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then redefine the **cost function** of linear regression as follows: \n",
    "\n",
    "$$\\displaystyle J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^m \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n w_j^2$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the partial derivatives with respect to $w_j$ and $b$ are: \n",
    "\n",
    "$$\\displaystyle \\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right) x_j^{(i)} + \\frac{\\lambda}{m} w_j \\ \\ \\ \\ for \\ j = 1, \\ldots, n$$ \n",
    "\n",
    "$$\\displaystyle \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are updated as follows in code, remember that this is the **gradient descent** algorithm, the learning rate $\\alpha$ is a hyperparameter that we need to tune.\n",
    "\n",
    "Also have in mind that the following are not equality assertions, but variable assignments, and that the weights are updated simultaneously.\n",
    "\n",
    "$$\\displaystyle w_j = w_j - \\alpha \\frac{\\partial J}{\\partial w_j} = w_j - \\alpha \\left(\\frac{1}{m} \\sum_{i=1}^m \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right) x_j^{(i)} + \\frac{\\lambda}{m} w_j\\right)$$ \n",
    "\n",
    "$$\\displaystyle b = b - \\alpha \\frac{\\partial J}{\\partial b} = b - \\alpha \\left(\\frac{1}{m} \\sum_{i=1}^m \\left(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)}\\right)\\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a linear regression auxiliary function \n",
    "\n",
    "def linear_reg(X, w, b):\n",
    "    return np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the updated cost function\n",
    "\n",
    "def J_linear_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    n = w.shape[0]\n",
    "\n",
    "    cost = 0. \n",
    "    loss = 0.\n",
    "    reg_cost = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        y_hat_i = linear_reg(X[i], w, b) \n",
    "        loss += (y_hat_i - y[i])**2\n",
    "    loss /= (2*m)\n",
    "\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j])**2 \n",
    "    reg_cost *= (lambda_ / (2*m)) \n",
    "\n",
    "    cost = loss + reg_cost\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this with some random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07917239320214275"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5, 6)\n",
    "y_tmp = np.array([0, 1, 0, 1, 0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1, ) - 0.5 \n",
    "b_tmp = 0.5 \n",
    "lambda_tmp = 0.7 \n",
    "\n",
    "cost_tmp = J_linear_reg(X=X_tmp, y=y_tmp, w=w_tmp, b=b_tmp, lambda_=lambda_tmp)\n",
    "\n",
    "cost_tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
